{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "from util import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import warnings\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import defaultdict\n",
    "from icecream import ic\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize;\n",
    "from pandarallel import pandarallel\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"precision\", 3)\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Read clean data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from \"../data/data_preprocessed.pickle\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['df_words_tkn', 'dict_ngrams', 'df_original'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = '../data/data_preprocessed.pickle'\n",
    "data = read_from_pickle(fn)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Remove unnecessary words**\n",
    "These words are either the highest level topics or unnecessary. And they largely change modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = data['df_original']\n",
    "df_words_tkn = data['df_words_tkn']\n",
    "dict_ngrams = data['dict_ngrams']\n",
    "remove_lst = ['mild traumatic brain', 'patient severe traumatic',\n",
    "              'severe traumatic brain', \n",
    "              'injury traumatic brain', 'brain injury patient',\n",
    "              'brain injury traumatic', 'traumatic brain injury',\n",
    "              'central nervous system', 'patient traumatic brain'\n",
    "             ]\n",
    "for w in remove_lst:\n",
    "    try:\n",
    "        dict_ngrams['trigrams'].remove(w)\n",
    "    except:\n",
    "        print(w)\n",
    "\n",
    "remove_lst = ['brain injury', 'traumatic brain', 'head injury',\n",
    "              'mild traumatic', 'long term', 'severe traumatic',\n",
    "              'post traumatic', 'patient traumatic', 'present study',\n",
    "              'head trauma', 'injury traumatic', 'brain damage','brain barrier',\n",
    "              'aim study', 'blood brain', 'scale score', 'systematic review',\n",
    "              'quality life'\n",
    "             ]\n",
    "for w in remove_lst:\n",
    "    try:\n",
    "        dict_ngrams['bigrams'].remove(w)\n",
    "    except:\n",
    "        print(w)\n",
    "    \n",
    "exception_dict = ['day', 'month', 'hour', 'le', 'case', 'time',\n",
    "                  'level', 'effect', 'data', 'change', 'analysis',\n",
    "                  'test', 'result', 'brain injury rat', 'outcome', 'finding', 'condition',\n",
    "                  'cost','muscle', 'artery','area', 'normal', 'role', \n",
    "                  'period', 'function', 'potential', 'region', 'use',\n",
    "                  'type', 'min', 'different', 'approach', 'method',\n",
    "                  'increase', 'decrease', 'reduce', 'low', 'site',\n",
    "                  'max', 'mean', 'higher', 'lower', 'measure',\n",
    "                  'total', 'activity','response', 'research',\n",
    "                  'non', 'current', 'specific', 'week', 'new', 'old',\n",
    "                  'task', 'work', 'evidence','management', 'common',\n",
    "                  'health', 'number','analysis', 'study','bbb',\n",
    "                  'aki', 'pre', 'bank', 'national', 'center', 'admission',\n",
    "                  'dc', 'pc', 'il', 'administration', 'addition', 'value',\n",
    "                  'early', 'greater', 'major', 'overall', 'related', 'single'\n",
    "                  'participant', 'individual', 'key', 'self', 'ratio', 'kg',\n",
    "                  'rate', 'cns', 'trauma', 'article'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Topic modeling using NMF**  \n",
    "\n",
    "Steps:\n",
    "- Merge n-grams\n",
    "- Vectorized the documents using TFIDF\n",
    "    - Set min_df and max_df to remove least anb most frequent terms\n",
    "- Train and NMF model ([Ref for parameters](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py))\n",
    "    - Use 5 topics for now. But need to identify the best number of topic for clustering in the future\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted models for years from 1991 to 1992\n",
      "fitted models for years from 1991 to 1996\n",
      "fitted models for years from 1991 to 2000\n",
      "fitted models for years from 1991 to 2004\n",
      "fitted models for years from 1991 to 2008\n",
      "fitted models for years from 1991 to 2012\n",
      "fitted models for years from 1991 to 2016\n",
      "fitted models for years from 1991 to 2020\n",
      "finished\n",
      "Saved data to \"../data/nmf_models_all_year.pickle\"\n"
     ]
    }
   ],
   "source": [
    "years = range(1991,2022)\n",
    "doc_topics_dict = defaultdict(list)\n",
    "vectorizer_dict = defaultdict(list)\n",
    "model_dict = defaultdict(list)\n",
    "for y in years:\n",
    "    if y % 4 == 0:\n",
    "        print('fitted models for years from {0} to {1}'\n",
    "              .format(1991, y))\n",
    "    # Consider n-grams so that the tokenizer does not process specific phrases\n",
    "    doc_word = df_words_tkn[y].parallel_apply(merge_ngrams, \n",
    "                                        dict_ngrams=dict_ngrams,\n",
    "                                        exception_dict=exception_dict).apply(','.join)\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = doc_word\n",
    "\n",
    "    n_topics = 5\n",
    "    t_vectorizer = TfidfVectorizer(stop_words = 'english',\n",
    "                                   tokenizer=dummy, # dummy tokenizer to maintain phrases\n",
    "                                   analyzer='word',\n",
    "                                   min_df=0.05,\n",
    "                                   max_df=0.95,                                   \n",
    "                                   )\n",
    "    doc_word = t_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    t_nmf_model = NMF(n_topics, random_state=15,\n",
    "                      beta_loss='kullback-leibler',\n",
    "                      solver='mu', max_iter=2000,\n",
    "                      alpha=0.1, l1_ratio=0.5,\n",
    "                      init='nndsvd'\n",
    "                     )\n",
    "    doc_topics_dict[y] = t_nmf_model.fit_transform(doc_word);\n",
    "    vectorizer_dict[y] = t_vectorizer\n",
    "    model_dict[y] = t_nmf_model\n",
    "    \n",
    "print('finished')\n",
    "fn = '../data/nmf_models_all_year.pickle'\n",
    "results_dict = {'doc_topics_dict': doc_topics_dict,\n",
    "                'vectorizer_dict': vectorizer_dict,\n",
    "                'model_dict': model_dict,\n",
    "               }\n",
    "save_as_pickle(fn, results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results and visualizations\n",
    "Refer to [../figures](https://github.com/weizhao-BME/metis-project4/tree/main/figures) for all the figures generated from this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Bargraph showing the weight of words for each topic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in years:\n",
    "    t_nmf_model = model_dict[y]\n",
    "    t_vectorizer = vectorizer_dict[y]\n",
    "    doc_topics = doc_topics_dict[y]\n",
    "    doc_cluster = doc_topics.argmax(axis = 1)\n",
    "    no_papers = pd.Series(doc_cluster).value_counts()\n",
    "\n",
    "    plot_top_words(t_nmf_model,\n",
    "                   t_vectorizer.get_feature_names(), 10, \n",
    "                   y, no_papers,\n",
    "                   figsize=(30, 20))\n",
    "    fn = '../figures/{0}_all_topics_bargraph.png'.format(y)\n",
    "    plt.savefig(fn, dpi=300, bbox_inches='tight', facecolor=\"white\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extract  all topics with 10 words for each from 1991 - 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_dict = store_topic_words(model_dict, vectorizer_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Word cloud figure for each topic and each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992\n",
      "1996\n",
      "2000\n",
      "2004\n",
      "2008\n",
      "2012\n",
      "2016\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "# word cloud\n",
    "from wordcloud import WordCloud\n",
    "# y=2021\n",
    "# n=0\n",
    "for y in years:\n",
    "    doc_topics = doc_topics_dict[y]\n",
    "    words = vectorizer_dict[y].get_feature_names()\n",
    "    t = model_dict[y].components_.argsort(axis=1)[:,-1:-8:-1]\n",
    "    topic_words = [[words[e] for e in l] for l in t]\n",
    "    if y%4 == 0:\n",
    "        print(y)\n",
    "    for n in range(0,len(topic_words)):\n",
    "        wc = WordCloud(background_color=\"white\",\n",
    "                       random_state=15,\n",
    "                       width=400,\n",
    "                       height=300,\n",
    "                      ).generate(','.join(topic_words[n]))\n",
    "        plt.subplots(figsize=(6, 6));\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\");\n",
    "        fn = '../figures/{0}_topics_{1}.png'.format(y, n)\n",
    "        plt.savefig(fn, dpi=300, bbox_inches='tight', facecolor=\"white\")\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Semantic similarity of topic words**\n",
    "To investigate the development of these identified topics, a semantic similarity analysis was performed. Using all the available documents, a word2vec model was trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [nondisclosure, collegiate, student, athlete, ...\n",
       "1    [extracellular, vesicle, concentration, glial,...\n",
       "2    [artificial, intelligence, report, fictional, ...\n",
       "3    [exo70, intracellular, redistribution, mild tr...\n",
       "4    [diffusion, tensor, neuropsychological, perfor...\n",
       "Name: title_and_abstract_lemma, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (df_original['title_and_abstract_lemma']\n",
    "      .parallel_apply(word_tokenize)\n",
    "      .parallel_apply(del_abbreviation)\n",
    "      .parallel_apply(merge_ngrams,\n",
    "                      dict_ngrams=dict_ngrams,\n",
    "                      exception_dict=exception_dict))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=3,\n",
    "                     size=500,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=12,\n",
    "                     seed=15)\n",
    "\n",
    "w2v_model.build_vocab(sentences=df.to_list(), progress_per=10000)\n",
    "w2v_model.train(sentences=df.to_list(),\n",
    "                total_examples=w2v_model.corpus_count,\n",
    "                epochs=50, report_delay=1)\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to \"../data/w2v_model.pickle\"\n"
     ]
    }
   ],
   "source": [
    "fn = '../data/w2v_model.pickle'\n",
    "save_as_pickle(fn, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity between each pair of topics (5x5=25 pairs) of previous and following years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_mat = np.zeros([n_topics, n_topics, len(years)])\n",
    "for idx_year, y in enumerate(years):\n",
    "    t1, t2 = topic_words_dict[y-1], topic_words_dict[y]\n",
    "    for idx_1, word_lst_1 in enumerate(t1):\n",
    "        for idx_2, word_lst_2 in enumerate(t2):\n",
    "            comparison_mat[idx_1, idx_2, idx_year] = (w2v_model\n",
    "                                                      .wv\n",
    "                                                      .n_similarity(word_lst_1,\n",
    "                                                                    word_lst_2))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each topic of previous years, identify its corresponding most similar topic of following years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.zeros([n_topics, len(years)], dtype=int)\n",
    "idx_max[:, 0] = np.arange(0, n_topics)\n",
    "max_score = np.zeros([n_topics, len(years)])\n",
    "for n in range(0, n_topics):\n",
    "    \n",
    "    for i in range(1, comparison_mat.shape[2]):\n",
    "        \n",
    "        idx_max[n, i] = comparison_mat[idx_max[n, i-1],:,i].argmax()\n",
    "        max_score[n, i] = comparison_mat[idx_max[n, i-1],:,i].max().round(2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic cosine similarity scores for each topic year by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = []\n",
    "for i, y in enumerate(years):\n",
    "    if i + 1 >=len(years):\n",
    "        break\n",
    "    else:\n",
    "        x_labels.append(str(y)+'-'+str(years[i+1]))\n",
    "\n",
    "fontdict={'fontsize': 15}\n",
    "for n in range(0, n_topics):\n",
    "    plt.subplots(figsize=(10, 5))\n",
    "    plt.title('Topic {}'.format(n), fontdict={'fontsize': 18})\n",
    "    ax = sns.barplot(x=list(range(1992, 2022)),\n",
    "                     y=max_score[n,1:],\n",
    "                     color=[0.2, 0.4, 0.6],\n",
    "                     edgecolor=(0,0,0),\n",
    "                    )\n",
    "    \n",
    "    ax.set_xticklabels(x_labels,\n",
    "                       rotation=90,\n",
    "                       fontdict={'fontsize': 13},\n",
    "                      );\n",
    "    ax.set_yticklabels(np.arange(0, 1.1, 0.2).round(1),\n",
    "                       fontdict=fontdict,\n",
    "                      );\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_xlim([-0.8,30.1])\n",
    "\n",
    "    [t_ax.set_width(1) for t_ax in ax.patches];\n",
    "    ax.set_xlabel('Years', fontdict=fontdict)\n",
    "    ax.set_ylabel('Semantic Cosine Similarity',\n",
    "                  fontdict=fontdict);\n",
    "\n",
    "    fn = '../figures/topic_{}_development.svg'.format(n)\n",
    "    plt.savefig(fn, dpi=300, bbox_inches='tight', facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
